{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EQWC0Kay0UfrSRIha2QBvoRBL6Q4MGzP",
      "authorship_tag": "ABX9TyOfsrCKvC/GTUv7KFrb31Au",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spindouken/holbertonschool-machine_learning/blob/master/unsupervised_learning/hyperparameter_tuning/MRIalz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alzheimer's Disease Classification using MRI Images and Bayesian Hyperparameter Optimization\n",
        "\n",
        "## Overview\n",
        "\n",
        "This project aims to develop a machine learning model to classify Alzheimer's disease based on MRI scans.\n",
        "\n",
        "## Technical Approach\n",
        "\n",
        "- **Data**: https://www.kaggle.com/datasets/sachinkumar413/alzheimer-mri-dataset/\n",
        "- **Model**: Convolutional Neural Network (CNN) based on MobileNetV2 architecture.\n",
        "- **Hyperparameter Tuning**: Bayesian Optimization using GPyOpt.\n",
        "- **Evaluation Metrics**: Accuracy, AUC-ROC, and F1-Score.\n",
        "\n",
        "## Sections\n",
        "\n",
        "1. **Imports**: Setting up the environment by importing necessary libraries.\n",
        "2. **Data Loading**: Function to load and preprocess the MRI images.\n",
        "3. **Model Creation**: Function to initialize and compile the CNN model.\n",
        "4. **Hyperparameter Space Definition**: Outlining the hyperparameters to be optimized.\n",
        "5. **Custom Metrics**: Including F1 Score as a custom evaluation metric.\n",
        "6. **Bayesian Optimization**: Objective function for the GPyOpt Bayesian optimizer.\n",
        "7. **Main Function**: Orchestrating the Bayesian optimization process.\n",
        "8. **Results Visualization**: Function to plot and save the optimization results.\n",
        "9. **Final Model Training**: Using the best hyperparameters to train the final model.\n",
        "\n",
        "## Usage\n",
        "\n",
        "Run each cell in sequence to go through the data loading, model creation, hyperparameter tuning, and final model training steps.\n",
        "\n",
        "## Contributors\n",
        "\n",
        "Mason counts\n",
        "\n",
        "## Last Updated\n",
        "\n",
        "10/17/23\n",
        "\n"
      ],
      "metadata": {
        "id": "IodRlqRzcYSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "need to install GPyOpt in colab environment and run imports for all files"
      ],
      "metadata": {
        "id": "l9SNdXxueK_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install GPyOpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoP5UqmrK8dT",
        "outputId": "93829d44-fbc6-4fad-c82c-32c847e3e11c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: GPyOpt in /usr/local/lib/python3.10/dist-packages (1.2.6)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from GPyOpt) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.10/dist-packages (from GPyOpt) (1.11.3)\n",
            "Requirement already satisfied: GPy>=1.8 in /usr/local/lib/python3.10/dist-packages (from GPyOpt) (1.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from GPy>=1.8->GPyOpt) (1.16.0)\n",
            "Requirement already satisfied: paramz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from GPy>=1.8->GPyOpt) (0.9.5)\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.10/dist-packages (from GPy>=1.8->GPyOpt) (3.0.3)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt) (4.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import GPyOpt\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.applications import DenseNet169\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "a_-xwPC0LKMl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === DATA LOADING FUNCTION ===\n",
        "# The load_data function is responsible for loading MRI images from a directory\n",
        "# It uses ImageDataGenerator to augment and split the data into training and validation subsets\n",
        "# The data (images) have already been preprocessed"
      ],
      "metadata": {
        "id": "Mn-_R0nNaEah"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e2Z7elkQ7HUg"
      },
      "outputs": [],
      "source": [
        "def load_data(data_dir, batch_size=32):\n",
        "    \"\"\"\n",
        "    load the Alzheimer MRI dataset from a specified directory\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): directory where the dataset is stored\n",
        "        batch_size (int): batch size for the data generator\n",
        "            will default to 32 if not specified when calling the function\n",
        "\n",
        "    Returns:\n",
        "        trainingGenerator, validationGenerator: data generators for training and validation sets\n",
        "    \"\"\"\n",
        "    datagen = ImageDataGenerator(validation_split=0.2)  # 20% data for validation\n",
        "\n",
        "    trainingGenerator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(128, 128),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    validationGenerator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(128, 128),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    return trainingGenerator, validationGenerator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === MODEL CREATION FUNCTION ===\n",
        "# The create_model function initializes a Convolutional Neural Network (CNN) using MobileNetV2 as the base model.\n",
        "# It adds a 'top model' for classification and returns the compiled model."
      ],
      "metadata": {
        "id": "N6fSemj2afQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model\n",
        "\"\"\"\n",
        "Use transfer learning to build on a pre-trained CNN\n",
        "    model for Alzheimer's classification\n",
        "\"\"\"\n",
        "def create_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Create the custom CNN model for Alzheimer's classification\n",
        "\n",
        "    Parameters:\n",
        "        input_shape (tuple): Shape of the input images.\n",
        "        num_classes (int): Number of classes in the dataset.\n",
        "    \"\"\"\n",
        "    # pre-trained DenseNet169 as the base model\n",
        "    base_model = DenseNet169(\n",
        "        include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
        "    )\n",
        "\n",
        "    # freeze all layers except the last two dense blocks\n",
        "    for layer in base_model.layers[:-17]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # get the output tensor of the base model\n",
        "    base_model_output = base_model.output\n",
        "\n",
        "    x = GlobalAveragePooling2D()(base_model_output)\n",
        "    x = Dense(512, activation=\"relu\")(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(256, activation=\"relu\")(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    # compile Model\n",
        "    model = Model(inputs=base_model.input, outputs=x)\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "cmRj1pDXK4Q1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === HYPERPARAMETER SPACE DEFINITION ===\n",
        "# The define_hyperparameter_space function outlines the hyperparameters to be optimized.\n",
        "# It returns a list of dictionaries, each specifying the name, type, and domain of a hyperparameter."
      ],
      "metadata": {
        "id": "peUfrCymajOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import GPyOpt\n",
        "\n",
        "def define_hyperparameter_space():\n",
        "    \"\"\"\n",
        "    Define the hyperparameter space for Bayesian optimization.\n",
        "\n",
        "    Returns:\n",
        "        domain (list): List of dictionaries specifying the hyperparameter space.\n",
        "    \"\"\"\n",
        "    domain = [\n",
        "        {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-4, 1e-2)},\n",
        "        {'name': 'dense_units', 'type': 'discrete', 'domain': (128, 256, 512)},\n",
        "        {'name': 'dropout_rate', 'type': 'continuous', 'domain': (0.3, 0.7)},\n",
        "        {'name': 'l2_weight', 'type': 'continuous', 'domain': (1e-5, 1e-3)},\n",
        "        {'name': 'batch_size', 'type': 'discrete', 'domain': (16, 32, 64)}\n",
        "    ]\n",
        "    return domain"
      ],
      "metadata": {
        "id": "XGkCaxqLKx02"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === F1 SCORE FUNCTION ===\n",
        "# Custom metric function for computing the F1 Score.\n",
        "# F1 Score is the harmonic mean of precision and recall and is crucial for binary classification tasks."
      ],
      "metadata": {
        "id": "fYYfFYlOa3da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "    return f1_val"
      ],
      "metadata": {
        "id": "OoBcib41a3CF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# === BAYESIAN OPTIMIZATION FUNCTION ===\n",
        "# The BayesianOptimization function serves as the objective function for GPyOpt.\n",
        "# It takes in hyperparameters, trains a model, evaluates it on the validation set,\n",
        "# and returns the validation loss."
      ],
      "metadata": {
        "id": "T0aYV3XVaprN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "iterationCount = 0\n",
        "bestValidationLoss = float('inf')\n",
        "bestF1Score = -1.0\n",
        "bestHyperparameters = None\n",
        "bestModel = None\n",
        "\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "    return f1_val\n",
        "\n",
        "def get_best_model():\n",
        "    global bestModel\n",
        "    return bestModel\n",
        "\n",
        "def get_best_hyperparameters():\n",
        "    global bestHyperparameters\n",
        "    return bestHyperparameters\n",
        "\n",
        "def BayesianOptimization(params):\n",
        "    \"\"\"\n",
        "    Creates model from create_model with training and validation data,\n",
        "        performs bayesian optimization on the model with the given hyperparameter space,\n",
        "        and returns the satisfacing metric.\n",
        "\n",
        "    Parameters:\n",
        "        params (dict): Hyperparameters to optimize.\n",
        "\n",
        "    Function is set to only save the best_model after training is complete\n",
        "    \"\"\"\n",
        "    global bestValidationLoss,bestHyperparameters, bestModel, bestF1Score, iterationCount\n",
        "\n",
        "    # extract hyperparameters from params\n",
        "    params = params[0]\n",
        "    learning_rate = params[0]\n",
        "    dense_units = int(params[1])\n",
        "    dropout_rate = params[2]\n",
        "    l2_weight = params[3]\n",
        "    batch_size = int(params[4])\n",
        "\n",
        "    hyperparameters = f\"lr={learning_rate:.5f}, du={dense_units}, dr={dropout_rate:.3f}, l2={l2_weight:.5f}, bs={batch_size}\"\n",
        "\n",
        "    iterationCount += 1\n",
        "    print(f\"Iteration: {iterationCount}\")\n",
        "    print(f\"Optimizing with: {hyperparameters}\")\n",
        "\n",
        "    # load training data\n",
        "    trainingGenerator, validationGenerator = load_data('/content/drive/MyDrive/MRIalz/Dataset/', batch_size=batch_size)\n",
        "\n",
        "    # create model from create_model.py\n",
        "    model = create_model((128, 128, 3), 4)  # Assuming 128x128 images and 4 classes\n",
        "\n",
        "    # compile model with new hyperparameters\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', AUC(name='auc'), f1_score])\n",
        "\n",
        "    # early_stopping will stop training if F1 score doesn't improve for # epochs (patience)\n",
        "    early_stopping = EarlyStopping(monitor='val_f1_score', patience=2, verbose=1, mode='min')\n",
        "\n",
        "    history = model.fit(trainingGenerator, epochs=1, validation_data=validationGenerator,\n",
        "                        batch_size=batch_size, callbacks=[early_stopping])\n",
        "\n",
        "    # return validation loss from the last epoch\n",
        "    currentValidationLoss = history.history['val_loss'][-1]\n",
        "    currentF1Score = history.history['val_f1_score'][-1]\n",
        "\n",
        "    # check if the current model performs better than the best model stored in memory\n",
        "    if currentF1Score > bestF1Score:\n",
        "        bestF1Score = currentF1Score\n",
        "        bestHyperparameters = hyperparameters\n",
        "        bestModel = model\n",
        "\n",
        "    # update best validation loss to be printed during training\n",
        "    if currentValidationLoss < bestValidationLoss:\n",
        "        bestValidationLoss = currentValidationLoss\n",
        "\n",
        "    print(f\"Validation loss with these parameters: {currentValidationLoss}\")\n",
        "    print(\"AUC-ROC:\", history.history['val_auc'][-1])\n",
        "    print(\"F1-Score:\", currentF1Score)\n",
        "    print(f\"Current best validation loss: {bestValidationLoss}, with hyperparameters: {bestHyperparameters}\")\n",
        "    print(f\"Current best F1 score: {bestF1Score}, with hyperparameters: {bestHyperparameters}\")\n",
        "\n",
        "    return currentF1Score\n"
      ],
      "metadata": {
        "id": "mz8th09yUrPx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === MAIN FUNCTION ===\n",
        "# Orchestrates the Bayesian optimization process.\n",
        "# It initializes the optimizer, runs it, and saves the best hyperparameters found during optimization."
      ],
      "metadata": {
        "id": "SVdDGGYra_bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import GPyOpt\n",
        "import pickle\n",
        "import datetime\n",
        "from tensorflow.keras.layers  import GlobalAveragePooling2D\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Run Bayesian optimization to tune hyperparameters using GPyOpt\n",
        "        and save the best hyperparameters (in best_params.pkl) to be used in training final_model.py\n",
        "        .pkl file will come with timestamp to account for multiple bayesian optimization runs\n",
        "\n",
        "    Main function utilizes the following functions to perform Bayesian optimization:\n",
        "        define_hyperparameter_space.py\n",
        "        BayesianOptimization.py\n",
        "        load_and_preprocess.py\n",
        "        save_and_plot.py\n",
        "\n",
        "    Note: bayesian optimization is actually performed in BayesianOptimization.py\n",
        "    \"\"\"\n",
        "    # create a directory for best models if it doesn't exist\n",
        "    if not os.path.exists('/content/drive/MyDrive/MRIalz/best_models'):\n",
        "        os.makedirs('/content/drive/MyDrive/MRIalz/best_models')\n",
        "\n",
        "    print(\"Starting Bayesian optimization...\")\n",
        "\n",
        "    # use function (from main folder)\n",
        "    #   which defined the hyperparameter space for Bayesian optimization\n",
        "    domainExpansion = define_hyperparameter_space()\n",
        "\n",
        "    # initialize bayesian optimization\n",
        "    # add initial_design_numdata=0 to avoid random initialization\n",
        "    #   and speed up optimization (for bug testing)\n",
        "    optimizer = GPyOpt.methods.BayesianOptimization(\n",
        "        f=BayesianOptimization,\n",
        "        domain=domainExpansion,\n",
        "        acquisition_type=\"EI\",  # expected improvement\n",
        "        exact_feval=True,\n",
        "        maximize=False,\n",
        "    )\n",
        "\n",
        "    # specify max run count for optimization\n",
        "    optimizer.run_optimization(max_iter=1)\n",
        "\n",
        "    best_model = get_best_model()\n",
        "    bestHyperparameters = get_best_hyperparameters()\n",
        "\n",
        "    if best_model is not None:\n",
        "        best_model.save(f\"best_models/bestModel_{bestHyperparameters}.h5\")\n",
        "\n",
        "    print(\n",
        "        \"Bayesian optimization completed. Next step: Use the best hyperparameters to train your final model.\"\n",
        "    )\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%m-%d-%y-%H:%M\")\n",
        "    filename = f\"best_params_{timestamp}.pkl\"\n",
        "    # retrieve best parameters from optimizer and save them to be used in final_model.py\n",
        "    best_params = optimizer.x_opt\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump(best_params, f)\n",
        "\n",
        "    # save and plot the results of the optimization\n",
        "    #   this will save the convergence plot as 'convergence.png'\n",
        "    #   and the optimization evaluations as 'bayes_opt_MRIalz.txt'\n",
        "    save_and_plot(optimizer)\n",
        "    print(\n",
        "        \"Best hyperparameters saved to best_params_{timestamp}.pkl. Convergence and acquisition visualizations were stored in their respective .png files.\"\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "uT2cWMSpK53A",
        "outputId": "2c65ca1a-b291-4d24-99da-fa843ec4147c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Bayesian optimization...\n",
            "Iteration: 1\n",
            "Optimizing with: lr=0.00869, du=128, dr=0.644, l2=0.00076, bs=16\n",
            "Found 5121 images belonging to 4 classes.\n",
            "Found 1279 images belonging to 4 classes.\n",
            "  2/321 [..............................] - ETA: 32:01 - loss: 11.3741 - accuracy: 0.3125 - auc: 0.5356 - f1_score: 0.2187     "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-16a29f01c409>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-16a29f01c409>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# add initial_design_numdata=0 to avoid random initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#   and speed up optimization (for bug testing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     optimizer = GPyOpt.methods.BayesianOptimization(\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdomainExpansion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, domain, constraints, cost_withGradients, model_type, X, Y, initial_design_numdata, initial_design_type, acquisition_type, normalize_Y, exact_feval, acquisition_optimizer_type, model_update_interval, evaluator_type, batch_size, num_cores, verbosity, verbosity_model, maximize, de_duplication, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0minitial_design_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_design_chooser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# --- CHOOSE the model type. If an instance of a GPyOpt model is passed (possibly user defined), it is used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m_init_design_chooser\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Case 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_procs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36m_eval_func\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mst_time\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mrlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mf_evals\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrlt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mcost_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-aebc2932cd45>\u001b[0m in \u001b[0;36mBayesianOptimization\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_f1_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     history = model.fit(trainingGenerator, epochs=1, validation_data=validationGenerator, \n\u001b[0m\u001b[1;32m     72\u001b[0m                         batch_size=batch_size, callbacks=[early_stopping])\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === PLOT AND SAVE RESULTS ===\n",
        "# The save_and_plot function visualizes the convergence of the Bayesian optimization process.\n",
        "# It also saves this and other evaluations into a text file."
      ],
      "metadata": {
        "id": "ieGFcbogbCyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import GPyOpt\n",
        "\n",
        "def save_and_plot(optimizer):\n",
        "    \"\"\"\n",
        "    Save evaluations and plot convergence.\n",
        "    \"\"\"\n",
        "    # plot and save convergence\n",
        "    optimizer.plot_convergence()\n",
        "    plt.savefig('/content/drive/MyDrive/MRIalz/convergence.png')\n",
        "\n",
        "    # save optimization evaluations to a text file\n",
        "    with open('/content/drive/MyDrive/MRIalz/bayes_opt_MRIalz.txt', 'w') as f:\n",
        "        f.write(str(optimizer.get_evaluations()))\n"
      ],
      "metadata": {
        "id": "Z0XeVkxXYpQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === FINAL MODEL TRAINING ===\n",
        "# The train_final_model function utilizes the best hyperparameters that were found during bayesian optimization to train the final model\n",
        "# It saves the best 'checkpoint' of the model training based on the validation loss"
      ],
      "metadata": {
        "id": "7Z_kQsTPbFte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "def train_final_model(best_params):\n",
        "    \"\"\"\n",
        "    Train the final model using the best hyperparameters.\n",
        "\n",
        "    Parameters:\n",
        "        best_params (dict): Best hyperparameters obtained from Bayesian optimization.\n",
        "    \"\"\"\n",
        "    print(\"Training final model with best hyperparameters...\")\n",
        "    # Extract best hyperparameters\n",
        "    learning_rate = best_params[0]\n",
        "    dense_units = int(best_params[1])\n",
        "    dropout_rate = best_params[2]\n",
        "    l2_weight = best_params[3]\n",
        "    batch_size = int(best_params[4])\n",
        "\n",
        "    # Load data\n",
        "    train_generator, val_generator = load_data('/content/drive/MyDrive/MRIalz/Dataset/')\n",
        "\n",
        "    # Create and compile model\n",
        "    model = create_model((128, 128, 3), 4)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Checkpoint to save the best model\n",
        "    checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/MRIalz/final_model.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_generator, epochs=10, validation_data=val_generator, batch_size=batch_size, callbacks=[checkpoint])\n",
        "\n",
        "    print(\"Final model trained and saved as 'final_model.h5'.\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/MRIalz/best_params.pkl\", \"rb\") as f:\n",
        "    best_params = pickle.load(f)\n",
        "\n",
        "train_final_model(best_params)\n"
      ],
      "metadata": {
        "id": "Pt64D0WnZhmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === Print Best_params ===\n",
        "# Print the best parameters from bayesian optimization\n",
        "# The parameters will be printed with their names according to the hyperparameters defined in define_hyperparameter_space function"
      ],
      "metadata": {
        "id": "OB5QBUi1bMzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/MRIalz/best_params.pkl', 'rb') as f:\n",
        "    # Load the data from the file\n",
        "    best_params = pickle.load(f)\n",
        "\n",
        "    # Get the hyperparameter names\n",
        "    hyperparameter_space = define_hyperparameter_space()\n",
        "    hyperparameter_names = [param['name'] for param in hyperparameter_space]\n",
        "\n",
        "    # Map names to best_params and print\n",
        "    named_best_params = dict(zip(hyperparameter_names, best_params))\n",
        "\n",
        "    print(\"Best Parameters:\")\n",
        "    for name, value in named_best_params.items():\n",
        "        print(f\"{name}: {value}\")\n"
      ],
      "metadata": {
        "id": "TroZ4L_aYxUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}